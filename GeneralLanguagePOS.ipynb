{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# !pip uninstall httpx httpcore\n",
    "# !pip install --upgrade httpx httpcore"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-13T15:38:35.121534Z",
     "start_time": "2024-07-13T15:38:35.070527Z"
    }
   },
   "id": "51e5c917ece26947",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import math\n",
    "import json\n",
    "import re\n",
    "import builtins\n",
    "import csv\n",
    "from deep_translator import GoogleTranslator"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:27:20.354539Z",
     "start_time": "2024-07-15T17:27:20.353399Z"
    }
   },
   "id": "bccb927f719ec96b",
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open('config.json') as config_file:\n",
    "    config = json.load(config_file)\n",
    "    open_api_key = config['open_api_key']\n",
    "client = OpenAI(api_key=open_api_key)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:27:20.859384Z",
     "start_time": "2024-07-15T17:27:20.811426Z"
    }
   },
   "id": "5fc9b97643525b85",
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['100-200.csv',\n '0-100.csv',\n '200-300.csv',\n '300-400.csv',\n '700-800.csv',\n '900-1000.csv',\n '500-600.csv',\n '400-500.csv',\n '600-700.csv',\n '800-900.csv',\n '1000-1250.csv',\n '1250-1500.csv',\n '1500-1750.csv',\n '1750-2000.csv']"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_files = ['100-200.csv', '0-100.csv', '200-300.csv', '300-400.csv', '700-800.csv', '900-1000.csv', '500-600.csv', '400-500.csv', '600-700.csv', '800-900.csv']\n",
    "selected_files += ['1000-1250.csv', '1250-1500.csv','1500-1750.csv',\n",
    "'1750-2000.csv']\n",
    "selected_files\n",
    "\n",
    "# selected_files = ['2000-2250.csv',\n",
    "# '2250-2500.csv',\n",
    "# '2500-2750.csv',\n",
    "# '2750-3000.csv']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:27:21.699923Z",
     "start_time": "2024-07-15T17:27:21.677514Z"
    }
   },
   "id": "f1e2db1f88551b47",
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "language = 'Dutch'\n",
    "language_code = 'nl'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:27:22.300460Z",
     "start_time": "2024-07-15T17:27:22.293284Z"
    }
   },
   "id": "debe05cbb35421f9",
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_sets_to_do(post_directory):\n",
    "    os.makedirs(os.path.join(language, 'Vocabulary', post_directory), exist_ok = True)\n",
    "    donesets = os.listdir(os.path.join(language, 'Vocabulary', post_directory))\n",
    "    print('Done:', donesets)\n",
    "    if '.DS_Store' in donesets:\n",
    "        donesets.remove('.DS_Store')\n",
    "    allsets = os.listdir(os.path.join(language, 'Vocabulary', 'split_sets'))\n",
    "    notdonesets = [i for i in allsets if not i in donesets]\n",
    "    todosets = notdonesets#['900-1000.csv']\n",
    "    if '.DS_Store' in donesets:\n",
    "        todosets.remove('.DS_Store')\n",
    "    print('To Do:', todosets)\n",
    "    return todosets\n",
    "# todosets = get_sets_to_do('Split_Sets','ChatGPT_Sets')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:27:51.700528Z",
     "start_time": "2024-07-15T17:27:51.670924Z"
    }
   },
   "id": "324822c4ae6b9b29",
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "      Unnamed: 0    Dutch  Frequency SpaCy_POS                      SpaCy_Tag  \\\n0              0       ik    7840843      PRON   VNW|pers|pron|nomin|vol|1|ev   \n1              1       je    7242582      PRON  VNW|pers|pron|nomin|red|2v|ev   \n2              2      het    5233420       DET              LID|bep|stan|evon   \n3              3       de    5171977       DET              LID|bep|stan|rest   \n4              4      dat    4505789     SCONJ                       VG|onder   \n...          ...      ...        ...       ...                            ...   \n3993        3993   gevolg       2399      NOUN      N|soort|ev|basis|onz|stan   \n3994        3994    troon       2399      NOUN     N|soort|ev|basis|zijd|stan   \n3995        3995      non       2399       SYM                       SPEC|afk   \n3996        3996  klappen       2398      NOUN               N|soort|mv|basis   \n3997        3997   zeuren       2396      VERB                   WW|pv|tgw|mv   \n\n              POS  Translation  \n0         Pronoun            I  \n1         Pronoun          you  \n2      Determiner           It  \n3      Determiner          the  \n4     Conjunction         that  \n...           ...          ...  \n3993         Noun  consequence  \n3994         Noun       throne  \n3995       Symbol          non  \n3996         Noun         clap  \n3997         Verb      to moan  \n\n[3998 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Dutch</th>\n      <th>Frequency</th>\n      <th>SpaCy_POS</th>\n      <th>SpaCy_Tag</th>\n      <th>POS</th>\n      <th>Translation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>ik</td>\n      <td>7840843</td>\n      <td>PRON</td>\n      <td>VNW|pers|pron|nomin|vol|1|ev</td>\n      <td>Pronoun</td>\n      <td>I</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>je</td>\n      <td>7242582</td>\n      <td>PRON</td>\n      <td>VNW|pers|pron|nomin|red|2v|ev</td>\n      <td>Pronoun</td>\n      <td>you</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>het</td>\n      <td>5233420</td>\n      <td>DET</td>\n      <td>LID|bep|stan|evon</td>\n      <td>Determiner</td>\n      <td>It</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>de</td>\n      <td>5171977</td>\n      <td>DET</td>\n      <td>LID|bep|stan|rest</td>\n      <td>Determiner</td>\n      <td>the</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>dat</td>\n      <td>4505789</td>\n      <td>SCONJ</td>\n      <td>VG|onder</td>\n      <td>Conjunction</td>\n      <td>that</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3993</th>\n      <td>3993</td>\n      <td>gevolg</td>\n      <td>2399</td>\n      <td>NOUN</td>\n      <td>N|soort|ev|basis|onz|stan</td>\n      <td>Noun</td>\n      <td>consequence</td>\n    </tr>\n    <tr>\n      <th>3994</th>\n      <td>3994</td>\n      <td>troon</td>\n      <td>2399</td>\n      <td>NOUN</td>\n      <td>N|soort|ev|basis|zijd|stan</td>\n      <td>Noun</td>\n      <td>throne</td>\n    </tr>\n    <tr>\n      <th>3995</th>\n      <td>3995</td>\n      <td>non</td>\n      <td>2399</td>\n      <td>SYM</td>\n      <td>SPEC|afk</td>\n      <td>Symbol</td>\n      <td>non</td>\n    </tr>\n    <tr>\n      <th>3996</th>\n      <td>3996</td>\n      <td>klappen</td>\n      <td>2398</td>\n      <td>NOUN</td>\n      <td>N|soort|mv|basis</td>\n      <td>Noun</td>\n      <td>clap</td>\n    </tr>\n    <tr>\n      <th>3997</th>\n      <td>3997</td>\n      <td>zeuren</td>\n      <td>2396</td>\n      <td>VERB</td>\n      <td>WW|pv|tgw|mv</td>\n      <td>Verb</td>\n      <td>to moan</td>\n    </tr>\n  </tbody>\n</table>\n<p>3998 rows Ã— 7 columns</p>\n</div>"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencydf = pd.read_csv(os.path.join('Frequency Lists', language, 'Clean.csv'))\n",
    "frequencydf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:27:52.269053Z",
     "start_time": "2024-07-15T17:27:52.242619Z"
    }
   },
   "id": "631a60b4b700101c",
   "execution_count": 65
  },
  {
   "cell_type": "markdown",
   "source": [
    "Clean"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "697070e6257d63fd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "VOCABULARY SPLIT"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d2c9d93cd2002f0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns: ['Dutch', 'POS', 'Translation']\n",
      "remainder= 248\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "def split_into_studysets(frequencydf, language, initial_splitsize=100, new_splitsize=250):\n",
    "    os.makedirs(os.path.join(language, 'Vocabulary', 'Split_Sets'), exist_ok=True)\n",
    "    columns = [language]\n",
    "    if 'POS' in frequencydf.columns:\n",
    "        columns.append('POS')\n",
    "    if 'Translation' in frequencydf.columns:\n",
    "        columns.append('Translation')\n",
    "    if 'English' in frequencydf.columns:\n",
    "        frequencydf.rename(columns={'English': 'Translation'}, inplace=True)\n",
    "        columns.append('Translation')\n",
    "    if 'lemma' in frequencydf.columns:\n",
    "        columns.append('show')\n",
    "        columns.append('lemma')\n",
    "    print('columns:', columns)\n",
    "    \n",
    "    total_rows = len(frequencydf)\n",
    "    i = 0\n",
    "    splitsize = initial_splitsize\n",
    "    \n",
    "    while i * splitsize < total_rows:\n",
    "        if i * splitsize >= 1000:\n",
    "            splitsize = new_splitsize\n",
    "            start_index = 1000 + (i - 10) * new_splitsize  # Adjust start index for new splitsize\n",
    "        else:\n",
    "            start_index = i * splitsize\n",
    "        \n",
    "        end_index = min(start_index + splitsize, total_rows)\n",
    "        subdf = frequencydf[start_index:end_index][columns]\n",
    "        subdf.to_csv(language + '/Vocabulary/Split_Sets/' + str(start_index) + '-' + str(end_index) + '.csv')\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    remainder = total_rows % splitsize\n",
    "    print('remainder=', remainder)\n",
    "    if remainder > 0:\n",
    "        remaindersubdf = frequencydf[-remainder:][columns]\n",
    "        remaindersubdf.to_csv(language + '/Vocabulary/Split_Sets/' + 'Remainder.csv')\n",
    "\n",
    "    return subdf\n",
    "\n",
    "# Example usage\n",
    "df = split_into_studysets(frequencydf, language=language)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:29:30.651277Z",
     "start_time": "2024-07-15T17:29:30.633314Z"
    }
   },
   "id": "58c4b9f973741e8e",
   "execution_count": 69
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you only want to do a subset of files for time purposes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87b4b10f96a58987"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def move_unselected_files(language, selected_files):\n",
    "    # Create directory for spare sets\n",
    "    spare_split_sets_dir = os.path.join(language, 'Vocabulary', 'Spare_Split_Sets')\n",
    "    os.makedirs(spare_split_sets_dir, exist_ok=True)\n",
    "\n",
    "    split_sets_dir = os.path.join(language, 'Vocabulary', 'Split_Sets')\n",
    "    all_files = os.listdir(split_sets_dir)\n",
    "    selected_files_set = set(selected_files)\n",
    "\n",
    "    # Move files not in selected_files to the spare sets directory\n",
    "    for file_name in all_files:\n",
    "        if file_name not in selected_files_set:\n",
    "            os.rename(\n",
    "                os.path.join(split_sets_dir, file_name),\n",
    "                os.path.join(spare_split_sets_dir, file_name)\n",
    "            )\n",
    "  # example of selected files\n",
    "move_unselected_files(language, selected_files)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:29:40.572075Z",
     "start_time": "2024-07-15T17:29:40.565073Z"
    }
   },
   "id": "8352a6d18e01aa0c",
   "execution_count": 70
  },
  {
   "cell_type": "markdown",
   "source": [
    "CHATGPT"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5484fa0c59fef9b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: ['.DS_Store', '800-900.csv', '100-200.csv', '600-700.csv', '700-800.csv', '0-100.csv', '300-400.csv', '900-1000.csv', '400-500.csv', '500-600.csv', '200-300.csv']\n",
      "To Do: ['1000-1250.csv', '1500-1750.csv', '1250-1500.csv', '1750-2000.csv']\n"
     ]
    }
   ],
   "source": [
    "CHATGPT_todosets = get_sets_to_do('ChatGPT_Sets')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:29:41.980665Z",
     "start_time": "2024-07-15T17:29:41.965989Z"
    }
   },
   "id": "c117c00eba8178c0",
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_sentence_in_target_language(variable_word, POS, words_to_include, model = 'gpt-4o-2024-05-13', tenses = ['present', 'future', 'past']):\n",
    "    \"\"\"\n",
    "    Generate a sentence in Russian using a specific word and part of speech, with words from a limited vocabulary, along with its English translation.\n",
    "\n",
    "    Parameters:\n",
    "    variable_word (str): The word to include in the sentence.\n",
    "    POS (str): The part of speech the word should operate as.\n",
    "    words_to_include (tuple): A list of words to try to include, and how many\n",
    "    model (str): The model to use for generating the sentence (default is 'gpt-4o-2024-05-13').\n",
    "    \n",
    "    # vocabulary (list): A list of words to use in the sentence.\n",
    "    # tenses (list)\n",
    "\n",
    "    Returns:\n",
    "    str: A sentence in Russian and its English translation, separated by a newline.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(variable_word, POS)\n",
    "    words_to_include = list(words_to_include[0].sample(words_to_include[1], replace=False))\n",
    "    print(words_to_include)\n",
    "    # vocabulary = vocabulary\n",
    "    # vocabulary_str = ', '.join(vocabulary)\n",
    "    prompt = (\n",
    "              # f\"Use only the following Russian words: {vocabulary_str}. \"\n",
    "        \n",
    "              f\"Create a simple sentence in f{language} using basic vocabulary and containing the word '{variable_word}' \"\n",
    "              f\"operating as a {POS} part of speech. Also, provide the English translation of the sentence separated by a newline.\"\n",
    "              f\"Try to include the following words in the sentence: {words_to_include}\"\n",
    "              # f\"Use only the following tenses: {tenses}. \"\n",
    "              )\n",
    "\n",
    "    try:\n",
    "        # Make the API request\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=50,\n",
    "            temperature=0.2,\n",
    "        )\n",
    "\n",
    "        # Extract and return the output\n",
    "        output_message = response.choices[0].message.content.strip()\n",
    "        return output_message\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "    # Extract and return the output\n",
    "    output_message = response.choices[0].message.content.strip()\n",
    "    return output_message"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:29:43.818257Z",
     "start_time": "2024-07-15T17:29:43.816756Z"
    }
   },
   "id": "6300157e037e763a",
   "execution_count": 72
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hoop Noun\n",
      "['haal', 'eigen']\n",
      "Ik haal mijn eigen hoop uit het leven.\n",
      "I draw my own hope from life.\n",
      "wou Verb\n",
      "['dagen', 'slecht']\n",
      "Ik wou dat de dagen niet zo slecht waren.\n",
      "I wished that the days were not so bad.\n",
      "vermoord Verb\n",
      "['kind', 'vroeg']\n",
      "De man vermoordde het kind vroeg in de ochtend.\n",
      "The man murdered the child early in the morning.\n",
      "elke Determiner\n",
      "['voel', 'kans']\n",
      "Elke kans voel ik.  \n",
      "Every opportunity I feel.\n"
     ]
    }
   ],
   "source": [
    "#TEST THE API\n",
    "for index, row in frequencydf.iloc[304:308].iterrows():\n",
    "    sentence = generate_sentence_in_target_language(row[language], row.POS, (frequencydf.iloc[300:400][language], 2))\n",
    "    print(sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:29:47.931053Z",
     "start_time": "2024-07-15T17:29:44.275876Z"
    }
   },
   "id": "f46e7071d2767627",
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def chatonset(s_set_df, set_name, n = 2):\n",
    "    words_to_include = (s_set_df[language], n)\n",
    "    s_set_df['ChatGPT_Sentence'] = s_set_df.apply(lambda row: generate_sentence_in_target_language(row[language], row['POS'], words_to_include), axis = 1)\n",
    "    s_set_df.to_csv(os.path.join(language, 'Vocabulary','ChatGPT_Sets', set_name))\n",
    "    return s_set_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:29:47.932966Z",
     "start_time": "2024-07-15T17:29:47.927564Z"
    }
   },
   "id": "initial_id",
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def do_all_sets(post_directory, func, language = language):\n",
    "    os.makedirs(os.path.join(language, 'Vocabulary', post_directory), exist_ok = True)\n",
    "    allsets = os.listdir(os.path.join(language, 'Vocabulary', 'split_sets'))\n",
    "    todosets = get_sets_to_do(post_directory)\n",
    "    for s_set in todosets:\n",
    "        if s_set[0] != '.':\n",
    "        # # if s_set == '500-600.csv':\n",
    "        #     # print(s_set)\n",
    "            s_set_df = pd.read_csv(\n",
    "                os.path.join(language, 'Vocabulary', 'split_sets', s_set)\n",
    "            )\n",
    "            print(s_set_df)\n",
    "            print(s_set)\n",
    "            func(s_set_df, s_set)\n",
    "    return"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:29:49.294709Z",
     "start_time": "2024-07-15T17:29:49.276366Z"
    }
   },
   "id": "431870784f63c5e0",
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: ['1000-1250.csv', '.DS_Store', '800-900.csv', '1500-1750.csv', '100-200.csv', '600-700.csv', '1250-1500.csv', '700-800.csv', '1750-2000.csv', '0-100.csv', '300-400.csv', '900-1000.csv', '400-500.csv', '500-600.csv', '200-300.csv']\n",
      "To Do: []\n"
     ]
    }
   ],
   "source": [
    "do_all_sets('ChatGPT_Sets', chatonset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:48:07.675552Z",
     "start_time": "2024-07-15T17:48:07.663211Z"
    }
   },
   "id": "4803e43262230313",
   "execution_count": 77
  },
  {
   "cell_type": "markdown",
   "source": [
    "ADD TRANSLATION INFORMATION:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c95b5e763d556a6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def add_set_translation_information(s_set_df, set_name, code = language_code):\n",
    "    chat = pd.read_csv(os.path.join(language, 'Vocabulary', 'ChatGPT_Sets', set_name))\n",
    "    # Apply the translation\n",
    "    if 'Translation' in chat.columns:\n",
    "        if 'POS' in chat.columns:\n",
    "            s_set_df = chat[[language, 'Translation']]\n",
    "        \n",
    "    else:\n",
    "        s_set_df['Translation'] = s_set_df.apply(\n",
    "            lambda row: GoogleTranslator(source= code, target='en').translate(row[language]), axis=1\n",
    "        )\n",
    "\n",
    "    # Save to CSV\n",
    "    s_set_df.to_csv(os.path.join(language, 'Vocabulary/Translated Sets', set_name), index=False)\n",
    "    return s_set_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:48:09.192527Z",
     "start_time": "2024-07-15T17:48:09.186791Z"
    }
   },
   "id": "12e9fbf3edff6829",
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: ['800-900.csv', '100-200.csv', '600-700.csv', '700-800.csv', '0-100.csv', '300-400.csv', '900-1000.csv', '400-500.csv', '500-600.csv', '200-300.csv']\n",
      "To Do: ['1000-1250.csv', '1500-1750.csv', '1250-1500.csv', '1750-2000.csv']\n"
     ]
    }
   ],
   "source": [
    "translation_todosets = get_sets_to_do('Translated Sets')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:48:10.112673Z",
     "start_time": "2024-07-15T17:48:10.104318Z"
    }
   },
   "id": "7d085b8180f44d76",
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000-1250.csv\n",
      "Done: ['800-900.csv', '100-200.csv', '600-700.csv', '700-800.csv', '0-100.csv', '300-400.csv', '900-1000.csv', '400-500.csv', '500-600.csv', '200-300.csv']\n",
      "To Do: ['1000-1250.csv', '1500-1750.csv', '1250-1500.csv', '1750-2000.csv']\n",
      "     Unnamed: 0          Dutch      POS Translation\n",
      "0          1000           eind     Noun         end\n",
      "1          1001          deden     Verb         did\n",
      "2          1002          mijne  Pronoun        mine\n",
      "3          1003           gooi     Noun       throw\n",
      "4          1004           gast     Noun       guest\n",
      "..          ...            ...      ...         ...\n",
      "245        1245        procent     Noun    per cent\n",
      "246        1246  neergeschoten     Verb        shot\n",
      "247        1247          spoor     Noun       track\n",
      "248        1248      kilometer     Noun  kilometers\n",
      "249        1249        bezorgd     Verb     Worried\n",
      "\n",
      "[250 rows x 4 columns]\n",
      "1000-1250.csv\n",
      "     Unnamed: 0       Dutch        POS   Translation\n",
      "0          1500       halve  Adjective          half\n",
      "1          1501  verdwijnen       Verb  to disappear\n",
      "2          1502     gelogen       Verb          lied\n",
      "3          1503        past       Verb         suits\n",
      "4          1504   bespreken       Verb       discuss\n",
      "..          ...         ...        ...           ...\n",
      "245        1745    verstand       Noun          mind\n",
      "246        1746      stuurt       Verb         sends\n",
      "247        1747       snapt       Verb   understands\n",
      "248        1748    oplossen       Verb      dissolve\n",
      "249        1749     vaarwel       Noun      farewell\n",
      "\n",
      "[250 rows x 4 columns]\n",
      "1500-1750.csv\n",
      "     Unnamed: 0      Dutch     POS       Translation\n",
      "0          1250   opdracht    Noun               Job\n",
      "1          1251   gevangen    Verb          captured\n",
      "2          1252       hoek    Noun            corner\n",
      "3          1253  vanmorgen  Adverb      this morning\n",
      "4          1254  verandert    Verb           changes\n",
      "..          ...        ...     ...               ...\n",
      "245        1495      opzij  Adverb             aside\n",
      "246        1496     openen    Verb           to open\n",
      "247        1497     tanden    Noun             teeth\n",
      "248        1498   hersenen    Noun             brain\n",
      "249        1499   getuigen    Verb  to give evidence\n",
      "\n",
      "[250 rows x 4 columns]\n",
      "1250-1500.csv\n",
      "     Unnamed: 0         Dutch          POS   Translation\n",
      "0          1750  ongelofelijk    Adjective  unbelievable\n",
      "1          1751         video         Noun         video\n",
      "2          1752          Adam  Proper noun          Adam\n",
      "3          1753        kregen         Verb           got\n",
      "4          1754        bevalt         Verb       pleases\n",
      "..          ...           ...          ...           ...\n",
      "245        1995          stof         Noun          dust\n",
      "246        1996           pot         Noun           pot\n",
      "247        1997        eieren         Noun          Eggs\n",
      "248        1998    verdedigen         Noun     to defend\n",
      "249        1999        kanker         Noun        cancer\n",
      "\n",
      "[250 rows x 4 columns]\n",
      "1750-2000.csv\n",
      "1500-1750.csv\n",
      "Done: ['1000-1250.csv', '800-900.csv', '1500-1750.csv', '100-200.csv', '600-700.csv', '1250-1500.csv', '700-800.csv', '1750-2000.csv', '0-100.csv', '300-400.csv', '900-1000.csv', '400-500.csv', '500-600.csv', '200-300.csv']\n",
      "To Do: []\n",
      "1250-1500.csv\n",
      "Done: ['1000-1250.csv', '800-900.csv', '1500-1750.csv', '100-200.csv', '600-700.csv', '1250-1500.csv', '700-800.csv', '1750-2000.csv', '0-100.csv', '300-400.csv', '900-1000.csv', '400-500.csv', '500-600.csv', '200-300.csv']\n",
      "To Do: []\n",
      "1750-2000.csv\n",
      "Done: ['1000-1250.csv', '800-900.csv', '1500-1750.csv', '100-200.csv', '600-700.csv', '1250-1500.csv', '700-800.csv', '1750-2000.csv', '0-100.csv', '300-400.csv', '900-1000.csv', '400-500.csv', '500-600.csv', '200-300.csv']\n",
      "To Do: []\n"
     ]
    }
   ],
   "source": [
    "for set in translation_todosets:\n",
    "    print(set)\n",
    "    do_all_sets('Translated Sets', add_set_translation_information)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:48:10.595071Z",
     "start_time": "2024-07-15T17:48:10.560433Z"
    }
   },
   "id": "25893c8ba5ba6b07",
   "execution_count": 80
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make into Quizlet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78468a35bd76d7aa"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def make_quizlet_set(set_name):\n",
    "    df = pd.read_csv(os.path.join(language, 'Vocabulary', 'ChatGPT_Sets', set_name))\n",
    "    transdf = pd.read_csv(os.path.join(language, 'Vocabulary', 'Translated Sets', set_name))\n",
    "\n",
    "    df['ChatGPT_Sentence'] = df['ChatGPT_Sentence'].apply(lambda x: re.sub(r'\\n+', '\\n', x))\n",
    "\n",
    "    # Removing text within parentheses that start with 'see'\n",
    "    df['ChatGPT_Sentence'] = df['ChatGPT_Sentence'].str.replace(r'\\(see[^)]*\\)', '', regex=True)\n",
    "    # Removing any extra spaces that might result from the replacement\n",
    "    df['ChatGPT_Sentence'] = df['ChatGPT_Sentence'].str.strip()\n",
    "    df['quizlet'] = df['POS'] + ' : ' + '*' + transdf['Translation'] + '*' + '\\n' + df['ChatGPT_Sentence']\n",
    "\n",
    "    df = df[[language, 'quizlet']]\n",
    "    os.makedirs(os.path.join(language, 'Vocabulary', 'Quizlet Sets'), exist_ok=True)\n",
    "    output_path = os.path.join(language, 'Vocabulary', 'Quizlet Sets', set_name.replace('.csv', '.txt'))\n",
    "\n",
    "    # Saving the data to a tab-separated text file with 3 lines between each row\n",
    "    with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        for index, row in df.iterrows():\n",
    "            outfile.write(f\"{row[language]}\\t{row['quizlet']}\\n\\n\\n\")\n",
    "\n",
    "    print(f\"Text file saved to {output_path}\")\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:52:13.426196Z",
     "start_time": "2024-07-15T17:52:13.417594Z"
    }
   },
   "id": "6f5dba1a4a2efcc8",
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def make_reverse_quizlet_set(set_name):\n",
    "    df = pd.read_csv(os.path.join(language, 'Vocabulary', 'ChatGPT_Sets', set_name))\n",
    "    transdf = pd.read_csv(os.path.join(language, 'Vocabulary', 'Translated Sets', set_name))\n",
    "\n",
    "    df['ChatGPT_Sentence'] = df['ChatGPT_Sentence'].apply(lambda x: re.sub(r'\\n+', '\\n', x))\n",
    "\n",
    "    # Removing text within parentheses that start with 'see'\n",
    "    df['ChatGPT_Sentence'] = df['ChatGPT_Sentence'].str.replace(r'\\(see[^)]*\\)', '', regex=True).str.strip()\n",
    "    \n",
    "    df['quizlet_front'] = df['POS'] + ' : ' + '*' + transdf['Translation'] + '*'\n",
    "    df['quizlet_back'] =  '*' + df[language] + '*' + '\\n' + df['ChatGPT_Sentence']\n",
    "\n",
    "    df = df[['quizlet_front', 'quizlet_back']]\n",
    "    os.makedirs(os.path.join(language, 'Vocabulary', 'Reverse Quizlet Sets'), exist_ok=True)\n",
    "    output_path = os.path.join(language, 'Vocabulary', 'Reverse Quizlet Sets', set_name.replace('.csv', '.txt'))\n",
    "\n",
    "    # Saving the data to a tab-separated text file with 3 lines between each row\n",
    "    with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        for index, row in df.iterrows():\n",
    "            outfile.write(f\"{row['quizlet_front']}\\t{row['quizlet_back']}\\n\\n\\n\")\n",
    "\n",
    "    print(f\"Text file saved to {output_path}\")\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:52:14.312762Z",
     "start_time": "2024-07-15T17:52:14.300455Z"
    }
   },
   "id": "61730a4d0b6bb5b3",
   "execution_count": 90
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#test make_quizlet_set('800-900.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:52:15.084399Z",
     "start_time": "2024-07-15T17:52:15.072640Z"
    }
   },
   "id": "5546d1cab8dd4609",
   "execution_count": 91
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'0-100.csv',\n '100-200.csv',\n '1000-1250.csv',\n '1250-1500.csv',\n '1500-1750.csv',\n '1750-2000.csv',\n '200-300.csv',\n '300-400.csv',\n '400-500.csv',\n '500-600.csv',\n '600-700.csv',\n '700-800.csv',\n '800-900.csv',\n '900-1000.csv'}"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builtin_set = builtins.set\n",
    "a = builtin_set(os.listdir(os.path.join(language, 'Vocabulary', 'ChatGPT_Sets')))\n",
    "b = builtin_set(os.listdir(os.path.join(language, 'Vocabulary', 'Translated Sets')))\n",
    "quizlet_todosets = a.intersection(b)\n",
    "reverse_quizlet_todosets = a.intersection(b)\n",
    "quizlet_todosets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:52:15.559954Z",
     "start_time": "2024-07-15T17:52:15.553871Z"
    }
   },
   "id": "f317b89326596dd2",
   "execution_count": 92
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text file saved to Dutch/Vocabulary/Quizlet Sets/600-700.txt\n",
      "Text file saved to Dutch/Vocabulary/Quizlet Sets/700-800.txt\n",
      "Text file saved to Dutch/Vocabulary/Quizlet Sets/100-200.txt\n",
      "Text file saved to Dutch/Vocabulary/Quizlet Sets/300-400.txt\n",
      "Text file saved to Dutch/Vocabulary/Quizlet Sets/0-100.txt\n",
      "Text file saved to Dutch/Vocabulary/Quizlet Sets/1000-1250.txt\n",
      "Text file saved to Dutch/Vocabulary/Quizlet Sets/900-1000.txt\n",
      "Text file saved to Dutch/Vocabulary/Quizlet Sets/1500-1750.txt\n",
      "Text file saved to Dutch/Vocabulary/Quizlet Sets/1750-2000.txt\n",
      "Text file saved to Dutch/Vocabulary/Quizlet Sets/800-900.txt\n",
      "Text file saved to Dutch/Vocabulary/Quizlet Sets/1250-1500.txt\n",
      "Text file saved to Dutch/Vocabulary/Quizlet Sets/200-300.txt\n",
      "Text file saved to Dutch/Vocabulary/Quizlet Sets/400-500.txt\n",
      "Text file saved to Dutch/Vocabulary/Quizlet Sets/500-600.txt\n",
      "Text file saved to Dutch/Vocabulary/Reverse Quizlet Sets/600-700.txt\n",
      "Text file saved to Dutch/Vocabulary/Reverse Quizlet Sets/700-800.txt\n",
      "Text file saved to Dutch/Vocabulary/Reverse Quizlet Sets/100-200.txt\n",
      "Text file saved to Dutch/Vocabulary/Reverse Quizlet Sets/300-400.txt\n",
      "Text file saved to Dutch/Vocabulary/Reverse Quizlet Sets/0-100.txt\n",
      "Text file saved to Dutch/Vocabulary/Reverse Quizlet Sets/1000-1250.txt\n",
      "Text file saved to Dutch/Vocabulary/Reverse Quizlet Sets/900-1000.txt\n",
      "Text file saved to Dutch/Vocabulary/Reverse Quizlet Sets/1500-1750.txt\n",
      "Text file saved to Dutch/Vocabulary/Reverse Quizlet Sets/1750-2000.txt\n",
      "Text file saved to Dutch/Vocabulary/Reverse Quizlet Sets/800-900.txt\n",
      "Text file saved to Dutch/Vocabulary/Reverse Quizlet Sets/1250-1500.txt\n",
      "Text file saved to Dutch/Vocabulary/Reverse Quizlet Sets/200-300.txt\n",
      "Text file saved to Dutch/Vocabulary/Reverse Quizlet Sets/400-500.txt\n",
      "Text file saved to Dutch/Vocabulary/Reverse Quizlet Sets/500-600.txt\n"
     ]
    }
   ],
   "source": [
    "for s_set in quizlet_todosets:\n",
    "    make_quizlet_set(s_set)\n",
    "for s_set in reverse_quizlet_todosets:\n",
    "    make_reverse_quizlet_set(s_set)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:52:20.635289Z",
     "start_time": "2024-07-15T17:52:20.463062Z"
    }
   },
   "id": "82be2d1cb2d9c891",
   "execution_count": 93
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# def makequizlet_step2(input_file):\n",
    "#     output_file = input_file \n",
    "#     with open(input_file, 'r', newline='', encoding='utf-8') as infile:\n",
    "#         reader = csv.reader(infile)\n",
    "#         rows = list(reader)\n",
    "# \n",
    "#     # Write the output CSV file with extra newlines\n",
    "#     with open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "#         writer = csv.writer(outfile, quoting=csv.QUOTE_MINIMAL)\n",
    "#         for row in rows:\n",
    "#             writer.writerow(row)\n",
    "#             writer.writerow([])  # Add a blank row\n",
    "#             writer.writerow([])  # Add a blank row\n",
    "#             writer.writerow([])\n",
    "# \n",
    "#     print(f\"Modified CSV file saved as {output_file}\")\n",
    "#     \n",
    "# for file in os.listdir(os.path.join(language, 'Vocabulary/Quizlet Sets')):\n",
    "#     if file[0] != '.':\n",
    "#         print(file)\n",
    "#         makequizlet_step2(os.path.join(language, 'Vocabulary/Quizlet Sets/' + file))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:48:17.683039Z",
     "start_time": "2024-07-15T17:48:17.676373Z"
    }
   },
   "id": "ff44b0def52b0a5",
   "execution_count": 86
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import csv\n",
    "# def make_quizlet_set(set_name):\n",
    "#     df = pd.read_csv(os.path.join(language, 'Vocabulary', 'ChatGPT_Sets', set_name))\n",
    "#     transdf = pd.read_csv(os.path.join(language, 'Vocabulary', 'Translated Sets', set_name))\n",
    "#     \n",
    "#     df['ChatGPT_Sentence'] = df['ChatGPT_Sentence'].apply(lambda x: re.sub(r'\\n+', '\\n', x))\n",
    "#     \n",
    "#     # Removing text within parentheses that start with 'see'\n",
    "#     df['ChatGPT_Sentence'] = df['ChatGPT_Sentence'].str.replace(r'\\(see[^)]*\\)', '', regex=True)   \n",
    "#     # Removing any extra spaces that might result from the replacement\n",
    "#     df['ChatGPT_Sentence'] = df['ChatGPT_Sentence'].str.strip()\n",
    "#     df['quizlet'] = transdf['POS'] + ' : ' + '*' + transdf['Translation'] + '*' + '\\n\\n' + df['ChatGPT_Sentence']\n",
    "# \n",
    "# \n",
    "#     df = df[[language, 'quizlet']]\n",
    "#     os.makedirs(os.path.join(language, 'Vocabulary', 'Quizlet Sets'), exist_ok = True)\n",
    "#     output_path = os.path.join(language, 'Vocabulary', 'Quizlet Sets', set_name)\n",
    "#     df.to_csv(output_path, index=False, quoting=csv.QUOTE_MINIMAL)\n",
    "#     # df.to_csv(output_path, index=False, quoting=csv.QUOTE_NONE, escapechar = ddd)\n",
    "#     print(f\"CSV file saved to {output_path}\")\n",
    "#     return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:48:18.257773Z",
     "start_time": "2024-07-15T17:48:18.251956Z"
    }
   },
   "id": "e263a8d9ad0ef2dd",
   "execution_count": 87
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# def remove_quotes_from_csv(file_path):\n",
    "#     # Read the CSV file and remove quotes\n",
    "#     with open(file_path, 'r', newline='') as infile:\n",
    "#         reader = csv.reader(infile)\n",
    "#         rows = [[cell.replace('\"', '') for cell in row] for row in reader]\n",
    "#     \n",
    "#     # Write the cleaned data back to the CSV file\n",
    "#     with open(file_path, 'w', newline='') as outfile:\n",
    "#         writer = csv.writer(outfile)\n",
    "#         writer.writerows(rows)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:48:19.003002Z",
     "start_time": "2024-07-15T17:48:18.995073Z"
    }
   },
   "id": "35ff43a88bf60c4a",
   "execution_count": 88
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1b9b1c34cdf60d0b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# for file in os.listdir('Russian/Vocabulary/Quizlet Sets'):\n",
    "#     if file[0] != '.':\n",
    "#         print(file)\n",
    "#         makequizlet_step2('Russian/Vocabulary/Quizlet Sets/' + file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-13T15:52:01.603365Z",
     "start_time": "2024-07-13T15:52:01.593747Z"
    }
   },
   "id": "f1075c8836d2d749",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-13T15:52:01.604316Z",
     "start_time": "2024-07-13T15:52:01.595368Z"
    }
   },
   "id": "c804ac11b3f56330",
   "execution_count": 26
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# !pip uninstall httpx httpcore\n",
    "# !pip install --upgrade httpx httpcoreimport pandas as pd\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import math\n",
    "import json\n",
    "import re\n",
    "import builtins\n",
    "import csv\n",
    "from deep_translator import GoogleTranslator"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:27:20.354539Z",
     "start_time": "2024-07-15T17:27:20.353399Z"
    }
   },
   "id": "bccb927f719ec96b",
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#frequencydf should be a csv with columns: Language (e.g. 'Dutch'), 'POS' and for best results, 'Translation'\n",
    "language = 'Dutch'\n",
    "language_code = 'nl' #For the translation API\n",
    "frequencydf = pd.read_csv(os.path.join('Frequency Lists', language, 'Clean.csv'))\n",
    "frequencydf"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "828a367aa491b845"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open('config.json') as config_file:\n",
    "    config = json.load(config_file)\n",
    "    open_api_key = config['open_api_key']\n",
    "client = OpenAI(api_key=open_api_key)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:27:20.859384Z",
     "start_time": "2024-07-15T17:27:20.811426Z"
    }
   },
   "id": "5fc9b97643525b85",
   "execution_count": 60
  },
  {
   "cell_type": "markdown",
   "source": [
    "SPLIT THE VOCABULARY INTO GROUPS OF STUDYSETS - default is 100 per set -> 1000, and 250 per set after."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d2c9d93cd2002f0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns: ['Dutch', 'POS', 'Translation']\n",
      "remainder= 248\n"
     ]
    }
   ],
   "source": [
    "def split_into_studysets(frequencydf, language, initial_splitsize=100, new_splitsize=250):\n",
    "    os.makedirs(os.path.join(language, 'Vocabulary', 'Split_Sets'), exist_ok=True)\n",
    "    columns = [language]\n",
    "    if 'POS' in frequencydf.columns:\n",
    "        columns.append('POS')\n",
    "    if 'Translation' in frequencydf.columns:\n",
    "        columns.append('Translation')\n",
    "    if 'English' in frequencydf.columns:\n",
    "        frequencydf.rename(columns={'English': 'Translation'}, inplace=True)\n",
    "        columns.append('Translation')\n",
    "    if 'lemma' in frequencydf.columns:\n",
    "        columns.append('show')\n",
    "        columns.append('lemma')\n",
    "    print('columns:', columns)\n",
    "    \n",
    "    total_rows = len(frequencydf)\n",
    "    i = 0\n",
    "    splitsize = initial_splitsize\n",
    "    \n",
    "    while i * splitsize < total_rows:\n",
    "        if i * splitsize >= 1000:\n",
    "            splitsize = new_splitsize\n",
    "            start_index = 1000 + (i - 10) * new_splitsize  # Adjust start index for new splitsize\n",
    "        else:\n",
    "            start_index = i * splitsize\n",
    "        \n",
    "        end_index = min(start_index + splitsize, total_rows)\n",
    "        subdf = frequencydf[start_index:end_index][columns]\n",
    "        subdf.to_csv(language + '/Vocabulary/Split_Sets/' + str(start_index) + '-' + str(end_index) + '.csv')\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    remainder = total_rows % splitsize\n",
    "    print('remainder=', remainder)\n",
    "    if remainder > 0:\n",
    "        remaindersubdf = frequencydf[-remainder:][columns]\n",
    "        remaindersubdf.to_csv(language + '/Vocabulary/Split_Sets/' + 'Remainder.csv')\n",
    "\n",
    "    return subdf\n",
    "\n",
    "# Example usage\n",
    "df = split_into_studysets(frequencydf, language=language)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:29:30.651277Z",
     "start_time": "2024-07-15T17:29:30.633314Z"
    }
   },
   "id": "58c4b9f973741e8e",
   "execution_count": 69
  },
  {
   "cell_type": "markdown",
   "source": [
    "OPTIONAL: If you only want to do a subset of files for time purposes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87b4b10f96a58987"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "selected_files = ['100-200.csv', '0-100.csv', '200-300.csv', '300-400.csv', '700-800.csv', '900-1000.csv', '500-600.csv', '400-500.csv', '600-700.csv', '800-900.csv']\n",
    "selected_files += ['1000-1250.csv', '1250-1500.csv','1500-1750.csv',\n",
    "'1750-2000.csv']\n",
    "# selected_files = ['2000-2250.csv','2250-2500.csv','2500-2750.csv',2750-3000.csv']\n",
    "# selected_files for only doing a subset to save time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T21:53:51.970991Z",
     "start_time": "2024-07-15T21:53:51.963353Z"
    }
   },
   "id": "f1e2db1f88551b47",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def move_unselected_files(language, selected_files):\n",
    "    # Create directory for spare sets\n",
    "    spare_split_sets_dir = os.path.join(language, 'Vocabulary', 'Spare_Split_Sets')\n",
    "    os.makedirs(spare_split_sets_dir, exist_ok=True)\n",
    "\n",
    "    split_sets_dir = os.path.join(language, 'Vocabulary', 'Split_Sets')\n",
    "    all_files = os.listdir(split_sets_dir)\n",
    "    selected_files_set = set(selected_files)\n",
    "\n",
    "    # Move files not in selected_files to the spare sets directory\n",
    "    for file_name in all_files:\n",
    "        if file_name not in selected_files_set:\n",
    "            os.rename(\n",
    "                os.path.join(split_sets_dir, file_name),\n",
    "                os.path.join(spare_split_sets_dir, file_name)\n",
    "            )\n",
    "  # example of selected files\n",
    "move_unselected_files(language, selected_files)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:29:40.572075Z",
     "start_time": "2024-07-15T17:29:40.565073Z"
    }
   },
   "id": "8352a6d18e01aa0c",
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_sets_to_do(post_directory):\n",
    "    os.makedirs(os.path.join(language, 'Vocabulary', post_directory), exist_ok = True)\n",
    "    completed_sets = os.listdir(os.path.join(language, 'Vocabulary', post_directory))\n",
    "    allsets = os.listdir(os.path.join(language, 'Vocabulary', 'split_sets'))\n",
    "    incomplete_sets = [i for i in allsets if not i in completed_sets]\n",
    "    if '.DS_Store' in donesets:\n",
    "        incomplete_sets.remove('.DS_Store')\n",
    "    # print('Done:', completed_sets)\n",
    "    # print('To Do:', incomplete_sets)\n",
    "    return incomplete_sets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:27:51.700528Z",
     "start_time": "2024-07-15T17:27:51.670924Z"
    }
   },
   "id": "324822c4ae6b9b29",
   "execution_count": 64
  },
  {
   "cell_type": "markdown",
   "source": [
    "GPT4 - Add example sentences for each word"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5484fa0c59fef9b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: ['.DS_Store', '800-900.csv', '100-200.csv', '600-700.csv', '700-800.csv', '0-100.csv', '300-400.csv', '900-1000.csv', '400-500.csv', '500-600.csv', '200-300.csv']\n",
      "To Do: ['1000-1250.csv', '1500-1750.csv', '1250-1500.csv', '1750-2000.csv']\n"
     ]
    }
   ],
   "source": [
    "CHATGPT_todosets = get_sets_to_do('ChatGPT_Sets')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:29:41.980665Z",
     "start_time": "2024-07-15T17:29:41.965989Z"
    }
   },
   "id": "c117c00eba8178c0",
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_sentence_in_target_language(variable_word, POS, words_to_include, model = 'gpt-4o-2024-05-13', tenses = ['present', 'future', 'past']):\n",
    "    \"\"\"\n",
    "    Generate a sentence in Target Language using a specific word and part of speech, with words from a limited vocabulary, along with its English translation.\n",
    "\n",
    "    Parameters:\n",
    "    variable_word (str): The word to include in the sentence.\n",
    "    POS (str): The part of speech the word should operate as.\n",
    "    words_to_include (tuple): A list of words to try to include, and how many\n",
    "    model (str): The model to use for generating the sentence (default is 'gpt-4o-2024-05-13').\n",
    "    \n",
    "    # vocabulary (list): A list of words to use in the sentence.\n",
    "    # tenses (list)\n",
    "\n",
    "    Returns:\n",
    "    str: A sentence in Russian and its English translation, separated by a newline.\n",
    "    \"\"\"\n",
    "    \n",
    "    words_to_include = list(words_to_include[0].sample(words_to_include[1], replace=False))\n",
    "    # print(variable_word, POS)\n",
    "    # print('words_to_include:', words_to_include)\n",
    "    prompt = (\n",
    "              f\"Create a simple sentence in f{language} using basic vocabulary and containing the word '{variable_word}' \"\n",
    "              f\"operating as a {POS} part of speech. Also, provide the English translation of the sentence separated by a newline.\"\n",
    "              f\"Try to include the following words in the sentence: {words_to_include}\"\n",
    "              # f\"Use only the following tenses: {tenses}. \"\n",
    "              )\n",
    "\n",
    "    try:\n",
    "        # Make the API request\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=50,\n",
    "            temperature=0.2,\n",
    "        )\n",
    "\n",
    "        # Extract and return the output\n",
    "        output_message = response.choices[0].message.content.strip()\n",
    "        return output_message\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "    # Extract and return the output\n",
    "    output_message = response.choices[0].message.content.strip()\n",
    "    \n",
    "    return output_message"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:29:43.818257Z",
     "start_time": "2024-07-15T17:29:43.816756Z"
    }
   },
   "id": "6300157e037e763a",
   "execution_count": 72
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hoop Noun\n",
      "['haal', 'eigen']\n",
      "Ik haal mijn eigen hoop uit het leven.\n",
      "I draw my own hope from life.\n",
      "wou Verb\n",
      "['dagen', 'slecht']\n",
      "Ik wou dat de dagen niet zo slecht waren.\n",
      "I wished that the days were not so bad.\n",
      "vermoord Verb\n",
      "['kind', 'vroeg']\n",
      "De man vermoordde het kind vroeg in de ochtend.\n",
      "The man murdered the child early in the morning.\n",
      "elke Determiner\n",
      "['voel', 'kans']\n",
      "Elke kans voel ik.  \n",
      "Every opportunity I feel.\n"
     ]
    }
   ],
   "source": [
    "#TEST THE API\n",
    "for index, row in frequencydf.iloc[104:108].iterrows():\n",
    "    sentence = generate_sentence_in_target_language(row[language], row.POS, (frequencydf.iloc[100:200][language], 2))\n",
    "    print(sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:29:47.931053Z",
     "start_time": "2024-07-15T17:29:44.275876Z"
    }
   },
   "id": "f46e7071d2767627",
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def Apply_LLM_to_studyset(s_set_df, set_name, n = 2):\n",
    "    words_to_include = (s_set_df[language], n)\n",
    "    s_set_df['ChatGPT_Sentence'] = s_set_df.apply(lambda row: generate_sentence_in_target_language(row[language], row['POS'], words_to_include), axis = 1)\n",
    "    s_set_df.to_csv(os.path.join(language, 'Vocabulary','ChatGPT_Sets', set_name))\n",
    "    return s_set_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:29:47.932966Z",
     "start_time": "2024-07-15T17:29:47.927564Z"
    }
   },
   "id": "initial_id",
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def do_all_sets(post_directory, func, language = language):\n",
    "    \"\"\" post-directory: directory where sets are stored\n",
    "    func: function to be applied to study_sets\"\"\"\n",
    "    os.makedirs(os.path.join(language, 'Vocabulary', post_directory), exist_ok = True)\n",
    "    for s_set in get_sets_to_do(post_directory):\n",
    "        if s_set[0] != '.': #IGNORE .DSStore\n",
    "            s_set_df = pd.read_csv(\n",
    "                os.path.join(language, 'Vocabulary', 'split_sets', s_set)\n",
    "            )\n",
    "            print(s_set)\n",
    "            func(s_set_df, s_set)\n",
    "    return"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:29:49.294709Z",
     "start_time": "2024-07-15T17:29:49.276366Z"
    }
   },
   "id": "431870784f63c5e0",
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: ['1000-1250.csv', '.DS_Store', '800-900.csv', '1500-1750.csv', '100-200.csv', '600-700.csv', '1250-1500.csv', '700-800.csv', '1750-2000.csv', '0-100.csv', '300-400.csv', '900-1000.csv', '400-500.csv', '500-600.csv', '200-300.csv']\n",
      "To Do: []\n"
     ]
    }
   ],
   "source": [
    "do_all_sets('ChatGPT_Sets', Apply_LLM_to_studyset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:48:07.675552Z",
     "start_time": "2024-07-15T17:48:07.663211Z"
    }
   },
   "id": "4803e43262230313",
   "execution_count": 77
  },
  {
   "cell_type": "markdown",
   "source": [
    "ADD TRANSLATION INFORMATION TO SETS IF NECESSARY:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c95b5e763d556a6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def add_set_translation_information(s_set_df, set_name, code = language_code):\n",
    "    LLM_Set = pd.read_csv(os.path.join(language, 'Vocabulary', 'ChatGPT_Sets', set_name))\n",
    "    \n",
    "    # Check whether translation is already in the data\n",
    "    if 'Translation' in LLM_Set.columns:\n",
    "        if 'POS' in LLM_Set.columns:\n",
    "            s_set_df = LLM_Set[[language, 'Translation']]\n",
    "        \n",
    "    else:\n",
    "        s_set_df['Translation'] = s_set_df.apply(\n",
    "            lambda row: GoogleTranslator(source= code, target='en').translate(row[language]), axis=1\n",
    "        )\n",
    "\n",
    "    s_set_df.to_csv(os.path.join(language, 'Vocabulary/Translated Sets', set_name), index=False)\n",
    "    return s_set_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:48:09.192527Z",
     "start_time": "2024-07-15T17:48:09.186791Z"
    }
   },
   "id": "12e9fbf3edff6829",
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: ['800-900.csv', '100-200.csv', '600-700.csv', '700-800.csv', '0-100.csv', '300-400.csv', '900-1000.csv', '400-500.csv', '500-600.csv', '200-300.csv']\n",
      "To Do: ['1000-1250.csv', '1500-1750.csv', '1250-1500.csv', '1750-2000.csv']\n"
     ]
    }
   ],
   "source": [
    "translation_todosets = get_sets_to_do('Translated Sets')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:48:10.112673Z",
     "start_time": "2024-07-15T17:48:10.104318Z"
    }
   },
   "id": "7d085b8180f44d76",
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000-1250.csv\n",
      "Done: ['800-900.csv', '100-200.csv', '600-700.csv', '700-800.csv', '0-100.csv', '300-400.csv', '900-1000.csv', '400-500.csv', '500-600.csv', '200-300.csv']\n",
      "To Do: ['1000-1250.csv', '1500-1750.csv', '1250-1500.csv', '1750-2000.csv']\n",
      "     Unnamed: 0          Dutch      POS Translation\n",
      "0          1000           eind     Noun         end\n",
      "1          1001          deden     Verb         did\n",
      "2          1002          mijne  Pronoun        mine\n",
      "3          1003           gooi     Noun       throw\n",
      "4          1004           gast     Noun       guest\n",
      "..          ...            ...      ...         ...\n",
      "245        1245        procent     Noun    per cent\n",
      "246        1246  neergeschoten     Verb        shot\n",
      "247        1247          spoor     Noun       track\n",
      "248        1248      kilometer     Noun  kilometers\n",
      "249        1249        bezorgd     Verb     Worried\n",
      "\n",
      "[250 rows x 4 columns]\n",
      "1000-1250.csv\n",
      "     Unnamed: 0       Dutch        POS   Translation\n",
      "0          1500       halve  Adjective          half\n",
      "1          1501  verdwijnen       Verb  to disappear\n",
      "2          1502     gelogen       Verb          lied\n",
      "3          1503        past       Verb         suits\n",
      "4          1504   bespreken       Verb       discuss\n",
      "..          ...         ...        ...           ...\n",
      "245        1745    verstand       Noun          mind\n",
      "246        1746      stuurt       Verb         sends\n",
      "247        1747       snapt       Verb   understands\n",
      "248        1748    oplossen       Verb      dissolve\n",
      "249        1749     vaarwel       Noun      farewell\n",
      "\n",
      "[250 rows x 4 columns]\n",
      "1500-1750.csv\n",
      "     Unnamed: 0      Dutch     POS       Translation\n",
      "0          1250   opdracht    Noun               Job\n",
      "1          1251   gevangen    Verb          captured\n",
      "2          1252       hoek    Noun            corner\n",
      "3          1253  vanmorgen  Adverb      this morning\n",
      "4          1254  verandert    Verb           changes\n",
      "..          ...        ...     ...               ...\n",
      "245        1495      opzij  Adverb             aside\n",
      "246        1496     openen    Verb           to open\n",
      "247        1497     tanden    Noun             teeth\n",
      "248        1498   hersenen    Noun             brain\n",
      "249        1499   getuigen    Verb  to give evidence\n",
      "\n",
      "[250 rows x 4 columns]\n",
      "1250-1500.csv\n",
      "     Unnamed: 0         Dutch          POS   Translation\n",
      "0          1750  ongelofelijk    Adjective  unbelievable\n",
      "1          1751         video         Noun         video\n",
      "2          1752          Adam  Proper noun          Adam\n",
      "3          1753        kregen         Verb           got\n",
      "4          1754        bevalt         Verb       pleases\n",
      "..          ...           ...          ...           ...\n",
      "245        1995          stof         Noun          dust\n",
      "246        1996           pot         Noun           pot\n",
      "247        1997        eieren         Noun          Eggs\n",
      "248        1998    verdedigen         Noun     to defend\n",
      "249        1999        kanker         Noun        cancer\n",
      "\n",
      "[250 rows x 4 columns]\n",
      "1750-2000.csv\n",
      "1500-1750.csv\n",
      "Done: ['1000-1250.csv', '800-900.csv', '1500-1750.csv', '100-200.csv', '600-700.csv', '1250-1500.csv', '700-800.csv', '1750-2000.csv', '0-100.csv', '300-400.csv', '900-1000.csv', '400-500.csv', '500-600.csv', '200-300.csv']\n",
      "To Do: []\n",
      "1250-1500.csv\n",
      "Done: ['1000-1250.csv', '800-900.csv', '1500-1750.csv', '100-200.csv', '600-700.csv', '1250-1500.csv', '700-800.csv', '1750-2000.csv', '0-100.csv', '300-400.csv', '900-1000.csv', '400-500.csv', '500-600.csv', '200-300.csv']\n",
      "To Do: []\n",
      "1750-2000.csv\n",
      "Done: ['1000-1250.csv', '800-900.csv', '1500-1750.csv', '100-200.csv', '600-700.csv', '1250-1500.csv', '700-800.csv', '1750-2000.csv', '0-100.csv', '300-400.csv', '900-1000.csv', '400-500.csv', '500-600.csv', '200-300.csv']\n",
      "To Do: []\n"
     ]
    }
   ],
   "source": [
    "for set in translation_todosets:\n",
    "    print(set)\n",
    "    do_all_sets('Translated Sets', add_set_translation_information)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:48:10.595071Z",
     "start_time": "2024-07-15T17:48:10.560433Z"
    }
   },
   "id": "25893c8ba5ba6b07",
   "execution_count": 80
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make into Quizlet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78468a35bd76d7aa"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def make_quizlet_ready_set(set_name):\n",
    "    #For the quizlet sets, the rows are separated by 3 blank lines \n",
    "    df = pd.read_csv(os.path.join(language, 'Vocabulary', 'ChatGPT_Sets', set_name))\n",
    "    transdf = pd.read_csv(os.path.join(language, 'Vocabulary', 'Translated Sets', set_name))\n",
    "\n",
    "    df['ChatGPT_Sentence'] = df['ChatGPT_Sentence'].apply(lambda x: re.sub(r'\\n+', '\\n', x))\n",
    "    # For French - Removing text within parentheses that start with 'see'\n",
    "    df['ChatGPT_Sentence'] = df['ChatGPT_Sentence'].str.replace(r'\\(see[^)]*\\)', '', regex=True)\n",
    "    df['ChatGPT_Sentence'] = df['ChatGPT_Sentence'].str.strip()\n",
    "    \n",
    "    df['quizlet'] = df['POS'] + ' : ' + '*' + transdf['Translation'] + '*' + '\\n' + df['ChatGPT_Sentence']\n",
    "    df = df[[language, 'quizlet']]\n",
    "    \n",
    "    os.makedirs(os.path.join(language, 'Vocabulary', 'Quizlet Sets'), exist_ok=True)\n",
    "    output_path = os.path.join(language, 'Vocabulary', 'Quizlet Sets', set_name.replace('.csv', '.txt'))\n",
    "\n",
    "    # Saving the data to a tab-separated text file with 3 lines between each row\n",
    "    with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        for index, row in df.iterrows():\n",
    "            outfile.write(f\"{row[language]}\\t{row['quizlet']}\\n\\n\\n\")\n",
    "\n",
    "    print(f\"Text file saved to {output_path}\")\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:52:13.426196Z",
     "start_time": "2024-07-15T17:52:13.417594Z"
    }
   },
   "id": "6f5dba1a4a2efcc8",
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def make_English2Language_quizlet_set(set_name):\n",
    "    df = pd.read_csv(os.path.join(language, 'Vocabulary', 'ChatGPT_Sets', set_name))\n",
    "    transdf = pd.read_csv(os.path.join(language, 'Vocabulary', 'Translated Sets', set_name))\n",
    "\n",
    "    df['ChatGPT_Sentence'] = df['ChatGPT_Sentence'].apply(lambda x: re.sub(r'\\n+', '\\n', x))\n",
    "\n",
    "    # Removing text within parentheses that start with 'see'\n",
    "    df['ChatGPT_Sentence'] = df['ChatGPT_Sentence'].str.replace(r'\\(see[^)]*\\)', '', regex=True).str.strip()\n",
    "    \n",
    "    df['quizlet_front'] = df['POS'] + ' : ' + '*' + transdf['Translation'] + '*'\n",
    "    df['quizlet_back'] =  '*' + df[language] + '*' + '\\n' + df['ChatGPT_Sentence']\n",
    "\n",
    "    df = df[['quizlet_front', 'quizlet_back']]\n",
    "    os.makedirs(os.path.join(language, 'Vocabulary', 'Reverse Quizlet Sets'), exist_ok=True)\n",
    "    output_path = os.path.join(language, 'Vocabulary', 'Reverse Quizlet Sets', set_name.replace('.csv', '.txt'))\n",
    "\n",
    "    # Saving the data to a tab-separated text file with 3 lines between each row\n",
    "    with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        for index, row in df.iterrows():\n",
    "            outfile.write(f\"{row['quizlet_front']}\\t{row['quizlet_back']}\\n\\n\\n\")\n",
    "\n",
    "    print(f\"Text file saved to {output_path}\")\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:52:14.312762Z",
     "start_time": "2024-07-15T17:52:14.300455Z"
    }
   },
   "id": "61730a4d0b6bb5b3",
   "execution_count": 90
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'0-100.csv',\n '100-200.csv',\n '1000-1250.csv',\n '1250-1500.csv',\n '1500-1750.csv',\n '1750-2000.csv',\n '200-300.csv',\n '300-400.csv',\n '400-500.csv',\n '500-600.csv',\n '600-700.csv',\n '700-800.csv',\n '800-900.csv',\n '900-1000.csv'}"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# builtin_set = builtins.set\n",
    "LLM_sets = builtin_set(os.listdir(os.path.join(language, 'Vocabulary', 'ChatGPT_Sets')))\n",
    "Translated_Sets = builtin_set(os.listdir(os.path.join(language, 'Vocabulary', 'Translated Sets')))\n",
    "quizlet_todosets = reverse_quizlet_todosets = LLM_sets.intersection(Translated_Sets)\n",
    "\n",
    "for s_set in quizlet_todosets:\n",
    "    make_quizlet_ready_set(s_set)\n",
    "    make_English2Language_quizlet_set(s_set)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:52:15.559954Z",
     "start_time": "2024-07-15T17:52:15.553871Z"
    }
   },
   "id": "f317b89326596dd2",
   "execution_count": 92
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# !pip uninstall httpx httpcore\n",
    "# !pip install --upgrade httpx httpcore"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-12T22:11:55.933715Z",
     "start_time": "2024-07-12T22:11:55.926577Z"
    }
   },
   "id": "51e5c917ece26947",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import math\n",
    "import json\n",
    "import re"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bccb927f719ec96b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open('config.json') as config_file:\n",
    "    config = json.load(config_file)\n",
    "    open_api_key = config['open_api_key']\n",
    "language = 'French'\n",
    "client = OpenAI(api_key=open_api_key)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-12T22:21:49.525076Z",
     "start_time": "2024-07-12T22:21:48.942938Z"
    }
   },
   "id": "debe05cbb35421f9",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100-200.csv', '0-100.csv']\n",
      "['800-900.csv', '600-700.csv', '700-800.csv', '300-400.csv', '900-1000.csv', '400-500.csv', '500-600.csv', '200-300.csv']\n"
     ]
    }
   ],
   "source": [
    "def get_sets_to_do(pre_directory, post_directory):\n",
    "    os.makedirs(os.path.join(language, 'Vocabulary', post_directory), exist_ok = True)\n",
    "    donesets = os.listdir(os.path.join(language, 'Vocabulary', post_directory))\n",
    "    print(donesets)\n",
    "    if '.DS_Store' in donesets:\n",
    "        donesets.remove('.DS_Store')\n",
    "    allsets = os.listdir(os.path.join(language, 'Vocabulary', pre_directory))\n",
    "    notdonesets = [i for i in allsets if not i in donesets]\n",
    "    todosets = notdonesets#['900-1000.csv']\n",
    "    if '.DS_Store' in donesets:\n",
    "        todosets.remove('.DS_Store')\n",
    "    print(todosets)\n",
    "    return todosets\n",
    "\n",
    "todosets = get_sets_to_do('Split_Sets','ChatGPT_Sets')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-12T23:08:17.765948Z",
     "start_time": "2024-07-12T23:08:17.745554Z"
    }
   },
   "id": "324822c4ae6b9b29",
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "source": [
    "Clean"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "697070e6257d63fd"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "     Rank    French                                 English  \\\n0       1      être                            to be; being   \n1       2     avoir                                 to have   \n2       3        je                                       I   \n3       4        de  of, from, by, than, in, with (see #28)   \n4       5        ne                                     not   \n..    ...       ...                                     ...   \n995   996         j                                     NaN   \n996   997    espoir                                     NaN   \n997   998     tueur                                     NaN   \n998   999   grandir                                     NaN   \n999  1000  dimanche                                  Sunday   \n\n                            POS  \n0    verb, auxiliary verb, noun  \n1          verb, auxiliary verb  \n2              personal pronoun  \n3                   preposition  \n4                        adverb  \n..                          ...  \n995                        noun  \n996                        noun  \n997                        noun  \n998                        verb  \n999                        noun  \n\n[1000 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Rank</th>\n      <th>French</th>\n      <th>English</th>\n      <th>POS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>être</td>\n      <td>to be; being</td>\n      <td>verb, auxiliary verb, noun</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>avoir</td>\n      <td>to have</td>\n      <td>verb, auxiliary verb</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>je</td>\n      <td>I</td>\n      <td>personal pronoun</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>de</td>\n      <td>of, from, by, than, in, with (see #28)</td>\n      <td>preposition</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>ne</td>\n      <td>not</td>\n      <td>adverb</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>996</td>\n      <td>j</td>\n      <td>NaN</td>\n      <td>noun</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>997</td>\n      <td>espoir</td>\n      <td>NaN</td>\n      <td>noun</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>998</td>\n      <td>tueur</td>\n      <td>NaN</td>\n      <td>noun</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>999</td>\n      <td>grandir</td>\n      <td>NaN</td>\n      <td>verb</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>1000</td>\n      <td>dimanche</td>\n      <td>Sunday</td>\n      <td>noun</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean(language = language):\n",
    "    df = pd.read_csv(language +'/Vocabulary/French1000POS.csv', sep = ',')\n",
    "#     df = df[[language, 'POS', 'English']]\n",
    "#     df.to_csv(language +'/Vocabulary/Clean')\n",
    "    return df\n",
    "df = clean()\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-12T23:08:26.058972Z",
     "start_time": "2024-07-12T23:08:26.042466Z"
    }
   },
   "id": "4f919517ef6ead7b",
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "source": [
    "VOCABULARY SPLIT"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d2c9d93cd2002f0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remainder= 0\n"
     ]
    }
   ],
   "source": [
    "def split_into_studysets(df, language, splitsize = 100):\n",
    "    os.makedirs(os.path.join(language, 'Vocabulary', 'Split_Sets'), exist_ok = True)\n",
    "    frequencydf = df\n",
    "    # frequencydf['Frequency'] = frequencydf['Frequency'].astype(int)\n",
    "    # frequencydf['cumsum'] = frequencydf['Frequency'].cumsum()/(0.01 * frequencydf['Frequency'].sum())\n",
    "    columns = [language, 'POS']\n",
    "    if 'English' in df.columns:\n",
    "        columns.append('English')\n",
    "    for i in range(math.floor(len(frequencydf)/splitsize)):\n",
    "        subdf = frequencydf[splitsize*i: splitsize*(i+1)][columns]\n",
    "        subdf.to_csv(language +'/Vocabulary/Split_Sets/' + str(splitsize*i) + '-' + str(splitsize*(i+1)) + '.csv')\n",
    "    print('remainder=', -(len(frequencydf)%splitsize))\n",
    "    remainder = -(len(frequencydf)%splitsize)\n",
    "    if remainder > 0:\n",
    "        remaindersubdf = frequencydf[remainder:][columns]\n",
    "        if len(remaindersubdf) != splitsize:\n",
    "            remaindersubdf.to_csv(language + '/Vocabulary/Split_Sets/' + 'Remainder.csv')\n",
    "\n",
    "    return subdf\n",
    "df = split_into_studysets(df, language = language)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-12T23:08:40.783908Z",
     "start_time": "2024-07-12T23:08:40.757170Z"
    }
   },
   "id": "58c4b9f973741e8e",
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "source": [
    "CHATGPT"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5484fa0c59fef9b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100-200.csv', '0-100.csv']\n",
      "['800-900.csv', '600-700.csv', '700-800.csv', '300-400.csv', '900-1000.csv', '400-500.csv', '500-600.csv', '200-300.csv']\n"
     ]
    }
   ],
   "source": [
    "todosets = get_sets_to_do('Split_Sets','ChatGPT_Sets')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-12T23:13:01.902840Z",
     "start_time": "2024-07-12T23:13:01.892558Z"
    }
   },
   "id": "c117c00eba8178c0",
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_sentence_in_target_language(variable_word, POS, words_to_include, model = 'gpt-4o-2024-05-13', tenses = ['present', 'future', 'past']):\n",
    "    \"\"\"\n",
    "    Generate a sentence in Russian using a specific word and part of speech, with words from a limited vocabulary, along with its English translation.\n",
    "\n",
    "    Parameters:\n",
    "    variable_word (str): The word to include in the sentence.\n",
    "    POS (str): The part of speech the word should operate as.\n",
    "    words_to_include (tuple): A list of words to try to include, and how many\n",
    "    model (str): The model to use for generating the sentence (default is 'gpt-4o-2024-05-13').\n",
    "    \n",
    "    # vocabulary (list): A list of words to use in the sentence.\n",
    "    # tenses (list)\n",
    "\n",
    "    Returns:\n",
    "    str: A sentence in Russian and its English translation, separated by a newline.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(variable_word)\n",
    "    words_to_include = list(words_to_include[0].sample(words_to_include[1], replace=False))\n",
    "    # vocabulary = vocabulary\n",
    "    # vocabulary_str = ', '.join(vocabulary)\n",
    "    prompt = (\n",
    "              # f\"Use only the following Russian words: {vocabulary_str}. \"\n",
    "        \n",
    "              f\"Create a simple sentence in f{language} using basic vocabulary and containing the word '{variable_word}' \"\n",
    "              f\"operating as a {POS} part of speech. Also, provide the English translation of the sentence separated by a newline.\"\n",
    "              f\"Try to include the following words in the sentence: {words_to_include}\"\n",
    "              # f\"Use only the following tenses: {tenses}. \"\n",
    "              )\n",
    "\n",
    "    try:\n",
    "        # Make the API request\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=50,\n",
    "            temperature=0.2,\n",
    "        )\n",
    "\n",
    "        # Extract and return the output\n",
    "        output_message = response.choices[0].message.content.strip()\n",
    "        return output_message\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "    # Extract and return the output\n",
    "    output_message = response.choices[0].message.content.strip()\n",
    "    return output_message\n",
    "# R1000 = pd.read_csv('Russian/Vocabulary/Clean')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-12T23:13:04.355806Z",
     "start_time": "2024-07-12T23:13:04.349555Z"
    }
   },
   "id": "6300157e037e763a",
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ', '.join(R1000.iloc[:100].Russian)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-12T23:13:06.225761Z",
     "start_time": "2024-07-12T23:13:06.209710Z"
    }
   },
   "id": "80539768f3abaa4c",
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# for index, row in R1000.iloc[:3].iterrows():\n",
    "#     print(row.Russian, row.POS)\n",
    "#     \n",
    "#     sentence = generate_sentence_in_russian(row.Russian, row.POS, (R1000.iloc[300:310].Russian, 2))\n",
    "#     print(sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-12T23:13:06.683512Z",
     "start_time": "2024-07-12T23:13:06.677482Z"
    }
   },
   "id": "f46e7071d2767627",
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def chatonset(s_set_df, set_name, n = 2):\n",
    "    words_to_include = (s_set_df[language], n)\n",
    "    s_set_df['ChatGPT_Sentence'] = s_set_df.apply(lambda row: generate_sentence_in_target_language(row[language], row['POS'], words_to_include), axis = 1)\n",
    "    s_set_df.to_csv(os.path.join(language, 'Vocabulary','ChatGPT_Sets', set_name))\n",
    "    return s_set_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-12T23:13:07.288366Z",
     "start_time": "2024-07-12T23:13:07.278753Z"
    }
   },
   "id": "initial_id",
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def do_all_sets(pre_directory, post_directory, func, language = language):\n",
    "    os.makedirs(os.path.join(language, 'Vocabulary', post_directory), exist_ok = True)\n",
    "    allsets = os.listdir(os.path.join(language, 'Vocabulary', pre_directory))\n",
    "    # todosets = get_sets_to_do(pre_directory, post_directory)\n",
    "    todosets = ['0-100.csv', '100-200.csv']\n",
    "    # print(allsets)\n",
    "    # print(todosets)\n",
    "    for s_set in todosets:\n",
    "        if s_set[0] != '.':\n",
    "        # if s_set == '500-600.csv':\n",
    "            # print(s_set)\n",
    "            s_set_df = pd.read_csv(\n",
    "                os.path.join(language, 'Vocabulary', pre_directory, s_set)\n",
    "            )\n",
    "            # print(s_set_df)\n",
    "            func(s_set_df, s_set)\n",
    "            \n",
    "    return"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-12T23:13:23.427432Z",
     "start_time": "2024-07-12T23:13:23.411863Z"
    }
   },
   "id": "431870784f63c5e0",
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "être\n",
      "avoir\n",
      "je\n",
      "de\n",
      "ne\n",
      "pas\n",
      "le\n",
      "la\n",
      "tu\n",
      "vous\n",
      "il\n",
      "et\n",
      "à\n",
      "un\n",
      "l'\n",
      "qui\n",
      "aller\n",
      "les\n",
      "en\n",
      "ça\n",
      "faire\n",
      "tout\n",
      "on\n",
      "que\n",
      "ce\n",
      "une\n",
      "mes\n",
      "d'\n",
      "pour\n",
      "se\n",
      "des\n",
      "dire\n",
      "pouvoir\n",
      "vouloir\n",
      "mais\n",
      "me\n",
      "nous\n",
      "dans\n",
      "elle\n",
      "savoir\n",
      "du\n",
      "où\n",
      "y\n",
      "t'\n",
      "bien\n",
      "voir\n",
      "plus\n",
      "non\n",
      "te\n",
      "mon\n",
      "au\n",
      "avec\n",
      "moi\n",
      "si\n",
      "quoi\n",
      "devoir\n",
      "oui\n",
      "ils\n",
      "comme\n",
      "s'\n",
      "venir\n",
      "sur\n",
      "toi\n",
      "ici\n",
      "rien\n",
      "lui\n",
      "bon\n",
      "là\n",
      "suivre\n",
      "pourquoi\n",
      "parler\n",
      "prendre\n",
      "cette\n",
      "quand\n",
      "alors\n",
      "une chose\n",
      "par\n",
      "son\n",
      "croire\n",
      "aimer\n",
      "falloir\n",
      "comment\n",
      "très\n",
      "ou\n",
      "passer\n",
      "penser\n",
      "aussi\n",
      "jamais\n",
      "attendre\n",
      "trouver\n",
      "laisser\n",
      "petit\n",
      "merci\n",
      "même\n",
      "sa\n",
      "ta\n",
      "autre\n",
      "arriver\n",
      "ces\n",
      "donner\n",
      "regarder\n",
      "encore\n",
      "appeler\n",
      "est-ce que\n",
      "peu\n",
      "homme\n",
      "partir\n",
      "ma\n",
      "toujours\n",
      "jour\n",
      "femme\n",
      "temps\n",
      "maintenant\n",
      "notre\n",
      "vie\n",
      "deux\n",
      "mettre\n",
      "rester\n",
      "sans\n",
      "seul\n",
      "arrêter\n",
      "vraiment\n",
      "connaître\n",
      "quelque\n",
      "sûr\n",
      "tuer\n",
      "mourir\n",
      "demander\n",
      "juste\n",
      "peut-être\n",
      "dieu\n",
      "fois\n",
      "oh\n",
      "père\n",
      "comprendre\n",
      "sortir\n",
      "personne\n",
      "an\n",
      "trop\n",
      "chez\n",
      "fille\n",
      "aux\n",
      "monde\n",
      "ami\n",
      "vrai\n",
      "après\n",
      "mal\n",
      "besoin\n",
      "accord\n",
      "ses\n",
      "avant\n",
      "monsieur\n",
      "enfant\n",
      "grand\n",
      "entendre\n",
      "voilà\n",
      "chercher\n",
      "heure\n",
      "mieux\n",
      "tes\n",
      "aider\n",
      "mère\n",
      "déjà\n",
      "beau\n",
      "essayer\n",
      "quel\n",
      "vos\n",
      "depuis\n",
      "quelqu'un\n",
      "beaucoup\n",
      "revenir\n",
      "donc\n",
      "plaire\n",
      "maison\n",
      "gens\n",
      "nuit\n",
      "ah\n",
      "soir\n",
      "nom\n",
      "bonjour\n",
      "jouer\n",
      "leur\n",
      "finir\n",
      "peur\n",
      "mort\n",
      "parce que\n",
      "perdre\n",
      "maman\n",
      "sentir\n",
      "ouais\n",
      "rentrer\n",
      "nos\n",
      "premier\n",
      "problème\n",
      "argent\n",
      "quelle\n",
      "vivre\n",
      "rendre\n",
      "dernier\n",
      "tenir\n"
     ]
    }
   ],
   "source": [
    "do_all_sets('Split_Sets', 'ChatGPT_Sets', chatonset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-12T23:15:51.766666Z",
     "start_time": "2024-07-12T23:13:38.914957Z"
    }
   },
   "id": "4803e43262230313",
   "execution_count": 64
  },
  {
   "cell_type": "markdown",
   "source": [
    "ADD TRANSLATION INFORMATION:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c95b5e763d556a6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-12T23:17:57.213507Z",
     "start_time": "2024-07-12T23:17:57.193468Z"
    }
   },
   "id": "4b30611e85cc8e7e",
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'fr'"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language[:2].lower()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-12T23:17:57.700903Z",
     "start_time": "2024-07-12T23:17:57.696607Z"
    }
   },
   "id": "3405403ef8f26292",
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def add_set_translation_information(s_set_df, set_name):\n",
    "    # Apply the translation\n",
    "    s_set_df['Translation'] = s_set_df.apply(\n",
    "        lambda row: GoogleTranslator(source= language[:2].lower(), target='en').translate(row[language]), axis=1\n",
    "    )\n",
    "\n",
    "    # Save to CSV\n",
    "    s_set_df.to_csv(os.path.join(language, 'Vocabulary/Translated Sets', set_name), index=False)\n",
    "    return s_set_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-12T23:00:14.843333Z",
     "start_time": "2024-07-12T23:00:14.824193Z"
    }
   },
   "id": "12e9fbf3edff6829",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0-100.csv']\n",
      "['100-200.csv']\n"
     ]
    }
   ],
   "source": [
    "translation_todosets = get_sets_to_do('ChatGPT_Sets', 'Translated Sets')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-12T23:00:20.871494Z",
     "start_time": "2024-07-12T23:00:20.842179Z"
    }
   },
   "id": "7d085b8180f44d76",
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100-200.csv\n"
     ]
    }
   ],
   "source": [
    "for set in translation_todosets:\n",
    "    print(set)\n",
    "    do_all_sets('ChatGPT_Sets', 'Translated Sets', add_set_translation_information)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-12T23:00:40.499618Z",
     "start_time": "2024-07-12T23:00:28.823012Z"
    }
   },
   "id": "25893c8ba5ba6b07",
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make into Quizlet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78468a35bd76d7aa"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def make_quizlet_set(df, set_name):\n",
    "    df['ChatGPT_Sentence'] = df['ChatGPT_Sentence'].apply(lambda x: re.sub(r'\\n+', '\\n', x))\n",
    "    \n",
    "    # Removing text within parentheses that start with 'see'\n",
    "    df['ChatGPT_Sentence'] = df['ChatGPT_Sentence'].str.replace(r'\\(see[^)]*\\)', '', regex=True)\n",
    "    \n",
    "    # Removing any extra spaces that might result from the replacement\n",
    "    df['ChatGPT_Sentence'] = df['ChatGPT_Sentence'].str.strip()\n",
    "\n",
    " \n",
    "    # Create the 'quizlet' column with POS : Translation and exactly two newlines between Translation and ChatGPT_Sentence\n",
    "    df['quizlet'] = df['POS'] + ' : ' + '*' + df['English'] + '*' + '\\n\\n' + df['ChatGPT_Sentence']\n",
    "\n",
    "\n",
    "    df = df[[language, 'quizlet']]\n",
    "    output_path = os.path.join(language, 'Vocabulary/Quizlet Sets', set_name)\n",
    "    df.to_csv(output_path, index=False, quoting=csv.QUOTE_MINIMAL)\n",
    "    # print(f\"CSV file saved to {output_path}\")\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-12T23:34:16.824008Z",
     "start_time": "2024-07-12T23:34:16.819577Z"
    }
   },
   "id": "d380c3c249815a47",
   "execution_count": 96
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store']\n",
      "['.DS_Store', '100-200.csv', '0-100.csv']\n",
      "100-200.csv\n",
      "Modified CSV file saved as French/Vocabulary/Quizlet Sets/100-200.csv\n",
      "0-100.csv\n",
      "Modified CSV file saved as French/Vocabulary/Quizlet Sets/0-100.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "quizlet_todosets = get_sets_to_do('ChatGPT_Sets', 'Quizlet Sets')\n",
    "for set in quizlet_todosets:\n",
    "    do_all_sets('ChatGPT_Sets', 'Quizlet Sets', make_quizlet_set)\n",
    "    \n",
    "def makequizlet_step2(input_file):\n",
    "    output_file = input_file \n",
    "    with open(input_file, 'r', newline='', encoding='utf-8') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        rows = list(reader)\n",
    "\n",
    "    # Write the output CSV file with extra newlines\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.writer(outfile, quoting=csv.QUOTE_MINIMAL)\n",
    "        for row in rows:\n",
    "            writer.writerow(row)\n",
    "            writer.writerow([])  # Add a blank row\n",
    "            writer.writerow([])  # Add a blank row\n",
    "            writer.writerow([])\n",
    "\n",
    "    print(f\"Modified CSV file saved as {output_file}\")\n",
    "    \n",
    "for file in os.listdir(os.path.join(language, 'Vocabulary/Quizlet Sets')):\n",
    "    if file[0] != '.':\n",
    "        print(file)\n",
    "        makequizlet_step2(os.path.join(language, 'Vocabulary/Quizlet Sets/' + file))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-12T23:34:17.727957Z",
     "start_time": "2024-07-12T23:34:17.696131Z"
    }
   },
   "id": "82be2d1cb2d9c891",
   "execution_count": 97
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# def remove_quotes_from_csv(file_path):\n",
    "#     # Read the CSV file and remove quotes\n",
    "#     with open(file_path, 'r', newline='') as infile:\n",
    "#         reader = csv.reader(infile)\n",
    "#         rows = [[cell.replace('\"', '') for cell in row] for row in reader]\n",
    "#     \n",
    "#     # Write the cleaned data back to the CSV file\n",
    "#     with open(file_path, 'w', newline='') as outfile:\n",
    "#         writer = csv.writer(outfile)\n",
    "#         writer.writerows(rows)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-12T23:29:56.491640Z",
     "start_time": "2024-07-12T23:29:56.476531Z"
    }
   },
   "id": "35ff43a88bf60c4a",
   "execution_count": 89
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1b9b1c34cdf60d0b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800-900.csv\n",
      "Modified CSV file saved as Russian/Vocabulary/Quizlet Sets/800-900.csv\n",
      "100-200.csv\n",
      "Modified CSV file saved as Russian/Vocabulary/Quizlet Sets/100-200.csv\n",
      "600-700.csv\n",
      "Modified CSV file saved as Russian/Vocabulary/Quizlet Sets/600-700.csv\n",
      "700-800.csv\n",
      "Modified CSV file saved as Russian/Vocabulary/Quizlet Sets/700-800.csv\n",
      "500-600.csv\n",
      "Modified CSV file saved as Russian/Vocabulary/Quizlet Sets/500-600.csv\n"
     ]
    }
   ],
   "source": [
    "# for file in os.listdir('Russian/Vocabulary/Quizlet Sets'):\n",
    "#     if file[0] != '.':\n",
    "#         print(file)\n",
    "#         makequizlet_step2('Russian/Vocabulary/Quizlet Sets/' + file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-12T23:29:57.535812Z",
     "start_time": "2024-07-12T23:29:57.458850Z"
    }
   },
   "id": "f1075c8836d2d749",
   "execution_count": 90
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c804ac11b3f56330"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
